---
layout: post
title:  "Can we scale up Transformers to AGI? No. But we still need them"
date:   2023-12-23
category: AI esseys
---
Release of GPT 3.5 (Chat-GPT) brought new life into "AGI" discussion. Small percentage of people would argue that we just acheived it (this percentage increased with release of GPT 4). Large group of AI enthusiasts started talking about timeline of achieving it (anything between 5 and 50 years). And some were still doubting that we are on track to the AGI. I think it is fair to say that results of scaling up transformer-based text models (GPTs) surprised most of us. A few months before it's release I was the first person to argue that generative models were not much more than "stochastic parrots"[1^] and predicting next word will always fail to mimic intelligence for it's lack of forward thinking and only relying on past outputs. It looks like I was, similarly to a lot of researchers, quite wrong.

[1^]: https://sciencebusiness.net/viewpoint/viewpoint-rooting-out-fake-papers-takes-patient-human-touch